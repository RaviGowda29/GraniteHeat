{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file loads the Granite 3.1 model and its tokenizer, tokenizes an input query, and ensures the tokens are moved to the correct device (CPU or MPS). It then generates a response from the model, decodes the output into human-readable text, and prints the result, with efficient device management for large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the 1 billion parameter model and tokenizer from Hugging Face using AutoModelForCausalLM and AutoTokenizer.\n",
    "- Automatically select the device (MPS for Apple Silicon or CPU) based on availability.\n",
    "- Implement memory constraints for both MPS and CPU to avoid exceeding 8GB of system memory.\n",
    "- Use accelerate's infer_auto_device_map and dispatch_model to optimize model layer distribution across devices.\n",
    "- Prevent layer splitting for specific module classes like GPTNeoXLayer to improve performance and stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravijgowda/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ravijgowda/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "\n",
    "# Load the pre-trained model weights using the Hugging Face Transformers library\n",
    "# \"ibm-granite/granite-3.1-1b-a400m-base\" is a model with 1 billion parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ibm-granite/granite-3.1-1b-a400m-base\")\n",
    "\n",
    "# Define the path to the model, used to load both model weights and tokenizer\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.1-1b-a400m-base\" \n",
    "\n",
    "# Load the tokenizer associated with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Check if the MPS (Metal Performance Shaders) backend is available, useful for Apple Silicon devices (like M1, M2 chips)\n",
    "# If MPS is available, use it, otherwise, fall back to CPU \n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device (MPS or CPU) based on availability\n",
    "model.to(device)\n",
    "\n",
    "# Define memory constraints for both MPS and CPU to avoid exceeding the system's memory limits (e.g., 4GB RAM limit)\n",
    "# The model with 1 billion parameters requires careful memory management\n",
    "max_memory = {\n",
    "    \"mps\": \"4GB\",  # Allocate up to 4GB for MPS (GPU)\n",
    "    \"cpu\": \"4GB\"   # Allocate up to 4GB for CPU\n",
    "}\n",
    "\n",
    "# Generate a device map for layer distribution across available devices (MPS and CPU)\n",
    "# The model's layers will be split intelligently to ensure that memory usage stays within the def\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added example for model inference with tokenization and text generation\n",
    "\n",
    "- Tokenized the input query using the model's tokenizer.\n",
    "- Moved input tokens to the same device as the model to ensure compatibility.\n",
    "- Generated output tokens using the model and specified a maximum output length.\n",
    "- Decoded the output tokens into human-readable text and printed the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the capital of India?\n",
      "\n",
      "The capital of India is New Delhi.\n",
      "\n",
      "What is the capital of the United States?\n",
      "\n",
      "The capital of the United States is Washington, D.C.\n",
      "\n",
      "What\n"
     ]
    }
   ],
   "source": [
    "# Import the tokenizer from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for the pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Input text for the model\n",
    "input_text = \"What's the capital of India\"\n",
    "\n",
    "# Tokenize input text and return as PyTorch tensors\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure input tokens are moved to the same device as the model\n",
    "input_tokens = {key: value.to(next(model.parameters()).device) for key, value in input_tokens.items()}\n",
    "\n",
    "# Generate output tokens from the model with a max length of 50\n",
    "output_tokens = model.generate(**input_tokens, max_length=50)\n",
    "\n",
    "# Decode the output tokens into readable text, skipping special tokens\n",
    "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated output\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
